Sender: LSF System <lsfadmin@lo-s4-049>
Subject: Job 1893267: <python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 512 --cell_size 512 --fidelity --continuity --log --cell_type lstm> in cluster <leonhard> Done

Job <python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 512 --cell_size 512 --fidelity --continuity --log --cell_type lstm> was submitted from host <lo-login-02> by user <rsikonja> in cluster <leonhard> at Sun May  5 20:34:59 2019
Job was executed on host(s) <6*lo-s4-049>, in queue <gpu.4h>, as user <rsikonja> in cluster <leonhard> at Sun May  5 20:35:22 2019
</cluster/home/rsikonja> was used as the home directory.
</cluster/home/rsikonja/MP/Slovenia> was used as the working directory.
Started at Sun May  5 20:35:22 2019
Terminated at Sun May  5 22:30:17 2019
Results reported at Sun May  5 22:30:17 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 512 --cell_size 512 --fidelity --continuity --log --cell_type lstm
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21597.20 sec.
    Max Memory :                                 5158 MB
    Average Memory :                             4831.57 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               7130.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                84
    Run time :                                   6915 sec.
    Turnaround time :                            6918 sec.

The output (if any) follows:

2019-05-05 20:35:27.798743: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-05 20:35:28.055747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:0f:00.0
totalMemory: 7.93GiB freeMemory: 7.82GiB
2019-05-05 20:35:28.055802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-05-05 20:35:28.711833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-05 20:35:28.711880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-05-05 20:35:28.711888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-05-05 20:35:28.712608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7307 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0f:00.0, compute capability: 6.1)
WARNING:tensorflow:From /cluster/home/rsikonja/.virtualenvs/mp-project/lib/python3.6/site-packages/tensorflow_gpu-1.12.0-py3.6-linux-x86_64.egg/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
