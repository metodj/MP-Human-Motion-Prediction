Sender: LSF System <lsfadmin@lo-s4-063>
Subject: Job 1893268: <python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 1024 --cell_size 1024 --fidelity --continuity --log --cell_type lstm> in cluster <leonhard> Done

Job <python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 1024 --cell_size 1024 --fidelity --continuity --log --cell_type lstm> was submitted from host <lo-login-02> by user <rsikonja> in cluster <leonhard> at Sun May  5 20:35:17 2019
Job was executed on host(s) <6*lo-s4-063>, in queue <gpu.4h>, as user <rsikonja> in cluster <leonhard> at Sun May  5 22:30:22 2019
</cluster/home/rsikonja> was used as the home directory.
</cluster/home/rsikonja/MP/Slovenia> was used as the working directory.
Started at Sun May  5 22:30:22 2019
Terminated at Mon May  6 00:37:38 2019
Results reported at Mon May  6 00:37:38 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python train.py --data_dir /cluster/project/infk/hilliges/lectures/mp19/project4 --save_dir ./experiments --experiment_name seq2seq_testing_gans --model_type seq2seq --residuals --loss geo --optimizer Adam --num_epochs 50 --input_hidden_size 1024 --cell_size 1024 --fidelity --continuity --log --cell_type lstm
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   16692.48 sec.
    Max Memory :                                 6659 MB
    Average Memory :                             5996.52 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               5629.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                72
    Run time :                                   7664 sec.
    Turnaround time :                            14541 sec.

The output (if any) follows:

2019-05-05 22:30:27.908033: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-05 22:30:28.211931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-05-05 22:30:28.211978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-05-05 22:30:29.995560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-05 22:30:29.995611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-05-05 22:30:29.995621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-05-05 22:30:29.996602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10060 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
WARNING:tensorflow:From /cluster/home/rsikonja/.virtualenvs/mp-project/lib/python3.6/site-packages/tensorflow_gpu-1.12.0-py3.6-linux-x86_64.egg/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
